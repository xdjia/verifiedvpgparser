{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from collections import defaultdict, namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ns2ms = 10**6\n",
    "\n",
    "def get_vpg_result(file):\n",
    "    Res = namedtuple('Res', 'parse extract tree')\n",
    "\n",
    "    res_vpg = {}\n",
    "    with open(file) as f:\n",
    "        text = f.readlines()\n",
    "\n",
    "    text = [row.split(\",\") for row in text[1:]]\n",
    "    text = [[t.strip() for t in row] for row in text]\n",
    "\n",
    "    for row in text:\n",
    "        k, *values = row\n",
    "        print(row)\n",
    "        res_vpg[k] = Res (*[float(v) for v in values])\n",
    "\n",
    "    return res_vpg\n",
    "\n",
    "def get_result(file):\n",
    "    with open(file) as f:\n",
    "        text = f.readlines()\n",
    "    text = [t.strip() for t in text]\n",
    "    names = text[0::3]\n",
    "    lex_times = [float(t) for t in text[1::3]]\n",
    "    parse_times = [float(t) for t in text[2::3]]\n",
    "    result = {}\n",
    "    for name, ltime, ptime in zip(names, lex_times, parse_times):\n",
    "        result[name] = (ltime, ptime)\n",
    "    return result\n",
    "\n",
    "def get_token(file):\n",
    "\n",
    "    with open(file) as f:\n",
    "        text = f.readlines()\n",
    "\n",
    "    text = [row.split(\",\") for row in text[1:]]\n",
    "    text = [[t.strip() for t in row] for row in text]\n",
    "\n",
    "    tokens = [(k, int(file_size), int(token_num)) for k, file_size, token_num in text]\n",
    "    # NOTE - sort by file size\n",
    "    tokens = sorted(tokens, key=lambda x:x[2])\n",
    "\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_vpg = get_vpg_result(\"../results/vpg.eval_xml.csv\")\n",
    "tokens = get_token(\"../token_info/xml.token_num\")\n",
    "res_antlr = get_result(\"../eval_antlr/xml/results/eval_antlr.xml.result\")\n",
    "res_bison = get_result(\"../eval_bison/xml/results/eval_bison.xml.result\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_file_size = False\n",
    "\n",
    "if use_file_size:\n",
    "    tokens = sorted(tokens, key=lambda x:x[1])\n",
    "\n",
    "    names, file_size, token_num = zip(*tokens[-5:])\n",
    "    names, file_size, token_num = zip(*tokens[2:])\n",
    "\n",
    "    sizes = file_size\n",
    "    size_name = \"File Size\"\n",
    "else:\n",
    "    tokens = sorted(tokens, key=lambda x:x[2])\n",
    "\n",
    "    names, file_size, token_num = zip(*tokens[-5:])\n",
    "    names, file_size, token_num = zip(*tokens[2:])\n",
    "    sizes = token_num\n",
    "    size_name = \"Number of Tokens\"\n",
    "\n",
    "x = sizes\n",
    "xi = list(range(len(x)))\n",
    "\n",
    "parse_antlr = [res_antlr[name][1] / ns2ms for name in names]\n",
    "parse_bison = [res_bison[name][1] / ns2ms for name in names]\n",
    "parse_vpg   = [res_vpg[name].parse / ns2ms for name in names]\n",
    "extract_vpg = [res_vpg[name].extract / ns2ms for name in names]\n",
    "pe_vpg = [(res_vpg[name].parse+res_vpg[name].extract) / ns2ms for name in names]\n",
    "tree_vpg    = [res_vpg[name].tree / ns2ms for name in names]\n",
    "sum_vpg      = [(res_vpg[name].parse+res_vpg[name].extract+res_vpg[name].tree) / ns2ms for name in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sizes\n",
    "xi = list(range(len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scienceplots\n",
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib.ticker as ticker\n",
    "\n",
    "\n",
    "with plt.style.context(['science','ieee', 'high-vis']):\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(x, parse_antlr, label=\"ANTLR\")\n",
    "    plt.plot(x, parse_bison, label=\"Bison\")\n",
    "    plt.plot(x, pe_vpg, label=\"VPG Parse + Extract\")\n",
    "    plt.plot(x, tree_vpg, label=\"VPG Conv\")\n",
    "    plt.plot(x, sum_vpg, label=\"VPG Sum\")\n",
    "    ax = plt.gca()\n",
    "    ax.set_xscale(\"log\")\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "    ax.set_xlabel(size_name)\n",
    "    ax.set_ylabel(\"Parsing Time (ms)\")\n",
    "    ax.set_title(\"Parsing XML\")\n",
    "    plt.legend()\n",
    "    if use_file_size:\n",
    "        plt.savefig('../figures/ParseXML_file_size.png')\n",
    "    else:\n",
    "        plt.savefig('../figures/ParseXML_num_tokens.png')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tables(names, sizes):\n",
    "    name_size = {name:size for name, size in zip(names,sizes)}\n",
    "    names = [ \"ORTCA\", \"SUAS\", \"address\", \"cd\", \"po\" ]\n",
    "    sizes = [name_size[name] for name in names ]\n",
    "\n",
    "    parse_antlr = [res_antlr[name][1] / ns2ms for name in names]\n",
    "    parse_bison = [res_bison[name][1] / ns2ms for name in names]\n",
    "    lex_antlr = [res_antlr[name][0] / ns2ms for name in names]\n",
    "    lex_bison = [res_bison[name][0] / ns2ms for name in names]\n",
    "    parse_vpg   = [res_vpg[name].parse / ns2ms for name in names]\n",
    "    extract_vpg = [res_vpg[name].extract / ns2ms for name in names]\n",
    "    pe_vpg = [(res_vpg[name].parse+res_vpg[name].extract) / ns2ms for name in names]\n",
    "    conv_vpg    = [res_vpg[name].tree / ns2ms for name in names]\n",
    "    sum_vpg      = [(res_vpg[name].parse+res_vpg[name].extract+res_vpg[name].tree) / ns2ms for name in names]\n",
    "\n",
    "    for i, (name,size) in enumerate(zip(names,sizes)):\n",
    "        print(( \"{} & {}\" + \" & {:.0f} ms\"*7 + \"\\\\\\\\\").format(name, size, parse_antlr[i],parse_bison[i],pe_vpg[i],conv_vpg[i], sum_vpg[i], lex_antlr[i], lex_bison[i]))\n",
    "\n",
    "    for i, (name,size) in enumerate(zip(names,sizes)):\n",
    "\n",
    "        print(( \"{}\"+\" & {:.0f} ms\"*4).format(size, lex_antlr[i], pe_vpg[i], sum_vpg[i], sum_vpg[i] + lex_antlr[i]))\n",
    "\n",
    "    for i, (name,size) in enumerate(zip(names,sizes)):\n",
    "        print(( \"{}\").format(size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5818ef7dbf71b66c02ba1f4ba4482e15ffadbdcc8e4360e167ad65ad9d088764"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
